{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's ensure we're using the correct Python environment\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "# Or in linux-based systems:\n",
    "# !which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy python-dotenv torch transformers\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here_is_my_secret_key\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check the environment variables\n",
    "print(os.getenv('SOME_SECRET_KEY'))\n",
    "print(os.getenv('ANOTHER_SECRET_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's go load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.mps.is_available())  # apple silicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    # Apple Silicon\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# device = 'cpu'  # force to cpu\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "llama_1b_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.bfloat16\n",
      "Time taken: 496.57046031951904 seconds\n",
      "[{'generated_text': \"Toktok, who is there? What do they do?\\nToktok is a popular American rapper, singer, songwriter, and record producer. He is the leader of the hip-hop group 21 Savage, and has also released solo music.\\n\\nI'm not sure if you're asking about Toktok or 21 Savage. Both are known for their unique styles and contributions to hip-hop.\\n\\nIf you could provide more context or clarify which Toktok you are referring to, I'd be happy to help further!\"}]\n"
     ]
    }
   ],
   "source": [
    "print(llama_1b_pipe.device)\n",
    "print(llama_1b_pipe.torch_dtype)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    result = llama_1b_pipe(\"Toktok, who is there?\", max_new_tokens=128)\n",
    "except KeyboardInterrupt:\n",
    "    result = None\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "torch.bfloat16\n",
      "Time taken: 4.574230909347534 seconds\n",
      "[{'generated_text': 'Toktok, who is there? I don’t know\\nI’m not aware of anyone named Toktok being in the news. Can you provide more context or clarify what you are referring to? I’m here to help with any questions you may have.'}]\n"
     ]
    }
   ],
   "source": [
    "print(llama_1b_pipe.device)\n",
    "print(llama_1b_pipe.torch_dtype)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    result = llama_1b_pipe(\"Toktok, who is there?\", max_new_tokens=128)\n",
    "except KeyboardInterrupt:\n",
    "    result = None\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TinyLlama 110M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Toktok, who is there? That means he was there.\\nHe had been walking on the beach when he heard a sound. It was a whistling. He looked around and saw a little bird. It was perched on a rock, whistling.\\nBrying was curious. He wanted to know why the bird was whistling.\\nSo he asked the bird, \"Why are you whistling?\"\\nThe bird said, \"I was just feeling a bit tired. I was flying all day and I needed a break.\"\\nBrying was surprised. He thought the bird was being selfish by not sharing the moment with'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"nickypro/tinyllama-110M\"\n",
    "\n",
    "tinyllama_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    ")\n",
    "\n",
    "tinyllama_pipe(\"Toktok, who is there?\", max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nickypro/tinyllama-110M\"\n",
    "\n",
    "tinyllama_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n",
      "torch.bfloat16\n",
      "Time taken: 4.106837034225464 seconds\n",
      "[{'generated_text': 'Toktok, who is there? He was at the park with his mom and dad. He had a big smile on his face because he was so excited to go on the slide.\\nHis mom said, \"Come on, let\\'s go get a ticket! We will go down the slide and then we will go get ice cream.\"\\nBaby was so excited and quickly ran over to the ticket booth. He asked the nice lady for a ticket, and she gave it to him.\\nBaby\\'s mom said, \"Hold on tight. That\\'s the slide.\"\\nBaby was so happy and he quickly ran over to'}]\n"
     ]
    }
   ],
   "source": [
    "print(tinyllama_pipe.device)\n",
    "print(tinyllama_pipe.torch_dtype)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    result = tinyllama_pipe(\"Toktok, who is there?\", max_new_tokens=128)\n",
    "except KeyboardInterrupt:\n",
    "    result = None\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: LMStudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-5stil8thtwn5r565z2vodk\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1748583675,\n",
      "  \"model\": \"llama-3.2-1b-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"{\\n   \\\"joke\\\": \\\"What do you call a fake noodle? An impasta.\\\"\\n}\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 47,\n",
      "    \"completion_tokens\": 21,\n",
      "    \"total_tokens\": 68\n",
      "  },\n",
      "  \"stats\": {},\n",
      "  \"system_fingerprint\": \"llama-3.2-1b-instruct\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:1234/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"model\": \"llama-3.2-1b-instruct\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful jokester.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a joke.\"\n",
    "        }\n",
    "    ],\n",
    "    \"response_format\": {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"joke_response\",\n",
    "            \"strict\": \"true\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"joke\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"joke\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 50,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
